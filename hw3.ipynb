{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode():\n",
    "    '''\n",
    "    Constructor for decision node\n",
    "    attr - attribute to be tested on\n",
    "    thresh - threshold for numerical attr, if categorical set to none\n",
    "    children - dict mapping attr value to child, if numeric child keys are 'le' and 'gr'\n",
    "    label - class label. default None, if not none, indicates that this is a leaf\n",
    "    '''\n",
    "    def __init__(self, attr, children, label=None, thresh=None):\n",
    "        self.attr = attr\n",
    "        self.children = children\n",
    "        self.label = label\n",
    "        self.thresh = thresh\n",
    "\n",
    "def learn_tree(data,attr_list,do_ig,attr_vals,targets,min_size_split=0,min_gain=0.0,max_depth=-1):\n",
    "    '''\n",
    "    Function to create decision tree based on data\n",
    "    do_ig - boolean, True to use info gain, False to use gini critereon\n",
    "    attr_vals - dict, maps to array of possible values for each attr, empty if numeric\n",
    "    targets - array, possible values for target class\n",
    "    set_depth - float, to set as the proportions of instances to create a leaf node\n",
    "    min_size_split - int, min number of instances required in data before returning a leaf node\n",
    "    min_gain - float, min info gain to determine when to stop\n",
    "    max_depth - int, if -1 no depth limit, else maximum depth before stopping \n",
    "    '''\n",
    "    #if num data too small, return leaf\n",
    "    if len(data) < min_size_split:\n",
    "        return DecisionNode(None,None,data['class'].unique()[0],None)\n",
    "    #if max depth reached, return leaf\n",
    "    if max_depth == 0:\n",
    "        return DecisionNode(None,None,data['class'].mode()[0],None)\n",
    "\n",
    "    #find best attr from list using info gain\n",
    "    max_gain = float('-inf')\n",
    "    min_gini = float('inf')\n",
    "    best_attr = ''\n",
    "\n",
    "    random_attr = sample(attr_list,int(math.sqrt(len(attr_list))))\n",
    "\n",
    "    for attr in random_attr:\n",
    "        if do_ig:\n",
    "            ig_attr,thresh = get_info_gain(data,attr,len(attr_vals),targets)\n",
    "            if ig_attr > max_gain:\n",
    "                max_gain = ig_attr\n",
    "                best_attr = attr\n",
    "                best_thresh = thresh\n",
    "        else:\n",
    "            gini_attr = get_critereon(data,attr)\n",
    "            if gini_attr < min_gini:\n",
    "                min_gini = gini_attr\n",
    "                best_attr = attr\n",
    "\n",
    "    #if info gainf rom split not sufficient, return leaf\n",
    "    if max_gain < min_gain:\n",
    "        return DecisionNode(None,None,data['class'].mode()[0],None)\n",
    "\n",
    "    best_vals = attr_vals[best_attr]\n",
    "    best_children = defaultdict(lambda: DecisionNode(None,None,data['class'].mode()[0],None))\n",
    "    \n",
    "    #if numeric attr\n",
    "    if len(best_vals) == 0:\n",
    "        best_children['le'] = learn_tree(data.loc[data[best_attr] <= best_thresh],attr_list,do_ig,attr_vals,targets,min_size_split,max_gain,max_depth-1) if len(data.loc[data[best_attr] <= best_thresh]) > 0 else DecisionNode(None,None,data['class'].mode()[0],None)\n",
    "        best_children['gr'] = learn_tree(data.loc[data[best_attr] > best_thresh],attr_list,do_ig,attr_vals,targets,min_size_split,max_gain,max_depth-1) if len(data.loc[data[best_attr] > best_thresh]) > 0 else DecisionNode(None,None,data['class'].mode()[0],None)\n",
    "    else:\n",
    "        for i in best_vals:\n",
    "            best_children[i] = learn_tree(data.loc[data[best_attr] == i],attr_list,do_ig,attr_vals,targets,min_size_split,max_gain,max_depth-1) if len(data.loc[data[best_attr] == i]) > 0 else DecisionNode(None,None,data['class'].mode()[0],None)\n",
    "\n",
    "    return DecisionNode(best_attr,best_children,thresh=best_thresh)\n",
    "\n",
    "\n",
    "#Helper functions for decision tree\n",
    "def get_info_gain(data,attr,num_cat,targets):\n",
    "    split_entropy = 0\n",
    "    if num_cat > 0:\n",
    "        best_split = None\n",
    "        for i in range(num_cat):\n",
    "            split_data = data.loc[[attr] == i]\n",
    "            split_entropy += (entropy(split_data,targets)*len(split_data))/len(data)\n",
    "    else:\n",
    "        sorted = data.sort_values(by=[attr])\n",
    "        sorted['mean'] = (sorted[attr] + sorted[attr].shift(-1))/2\n",
    "        sorted['entropy'] = sorted['mean'].apply(lambda row: split_thresh(sorted,attr,row,targets))\n",
    "        best_row = sorted.nsmallest(1,['entropy'])\n",
    "        best_split = best_row['mean']\n",
    "\n",
    "    return (entropy(data,targets) - split_entropy),best_split\n",
    "\n",
    "#determine split threshhold for numeric attr\n",
    "def split_thresh(data,attr,thresh,targets):\n",
    "    if math.isnan(thresh):\n",
    "        return entropy(data,targets)\n",
    "    df_le = data.loc[data[attr] <= thresh]\n",
    "    df_g = data.loc[data[attr] > thresh]\n",
    "    return (entropy(df_le,targets)*len(df_le) + entropy(df_g,targets)*len(df_g))/len(data)\n",
    "\n",
    "def entropy(data,targets):\n",
    "    if len(data) == 0:\n",
    "        return 0\n",
    "    ent = 0\n",
    "    for i in range(len(targets)):\n",
    "        try:\n",
    "            prob_i = data['class'].value_counts().iloc[i]/len(data)\n",
    "            ent -= (prob_i * math.log(prob_i,2))\n",
    "        except:\n",
    "            pass\n",
    "    return ent\n",
    "\n",
    "def get_critereon(data,attr):\n",
    "    data_0 = data.loc[data[attr] == 0]\n",
    "    data_1 = data.loc[data[attr] == 1]\n",
    "    data_2 = data.loc[data[attr] == 2]\n",
    "    return (gini(data_0)*len(data_0) + gini(data_1)*len(data_1) + gini(data_2)*len(data_2))/len(data)\n",
    "\n",
    "def gini(data):\n",
    "    if len(data) == 0:\n",
    "        return 0\n",
    "    prob_0 = data['class'].value_counts().iloc[0]/len(data)\n",
    "    crit = prob_0 ** 2\n",
    "    if len(data['class'].value_counts()) > 1:\n",
    "        prob_1 = data['class'].value_counts().iloc[1]/len(data)\n",
    "        crit += prob_1 ** 2\n",
    "    else:\n",
    "        pass\n",
    "    return crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train tree on training data\n",
    "wine_df = pd.read_csv('datasets/hw3_wine.csv',delim_whitespace=True)\n",
    "w_train,w_test = train_test_split(wine_df,test_size=0.2)\n",
    "wine_attr = defaultdict(list)\n",
    "wine_targets = [1,2,3]\n",
    "decision_tree = learn_tree(w_train,list(w_train.columns.values).remove('class'),True,wine_attr,wine_targets)\n",
    "\n",
    "#function to classify an instance based on decision tree\n",
    "def classify(instance):\n",
    "    node = decision_tree\n",
    "    while node.label is None:\n",
    "        if node.thresh is None:\n",
    "            node = node.children[instance[node.attr]]\n",
    "        else:\n",
    "            node = node.children['le'] if instance[node.attr] <= node.thresh else node.children['gr']\n",
    "    guess = node.label\n",
    "    actual = instance['class']\n",
    "    return guess\n",
    "\n",
    "#test accuracy on training data\n",
    "def test_decision(to_train,data):\n",
    "    predictions = to_train.apply(lambda row: classify(row), axis=1)\n",
    "    actual_labels = data.loc[predictions.index,['class']]\n",
    "    return predictions.eq(actual_labels['class'].values).mean()\n",
    "\n",
    "accuracy_test = []\n",
    "accuracy_train = []\n",
    "\n",
    "\n",
    "\n",
    "'''for i in range (100):\n",
    "    voter_train, voter_test = train_test_split(voter_df,test_size=0.2)\n",
    "    attributes = list(voter_train.columns.values)\n",
    "    attributes.remove('target')\n",
    "    decision_tree = learn_tree(voter_train,attributes,True)\n",
    "    accuracy_train.append(test_decision(voter_train))\n",
    "    accuracy_test.append(test_decision(voter_test))'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
